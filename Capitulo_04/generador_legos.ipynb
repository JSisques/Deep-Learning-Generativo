{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunto de datos Bricks\n",
    "\n",
    "Lo primero que deberemos hacer es descargar los datos paara el entrenamiento. Utilizaremos el conjunto de datos [Images of LEGO Bricks](https://www.kaggle.com/datasets/joosthazelzet/lego-brick-images), disponible en Kaggle. Se trata de una colección de 40.000 fotografias informatizadas de 50 ladrillos de juguete distintos, tomadas desde distintos ángulo.\n",
    "\n",
    "Utilizamos la función de Keras `image_dataset_from_directory` para crear un TensorFlow Dataset. De este modo podemos leer grupos de imágenes en la memoria solo cuando sea necesario (en el entrenamiento). Tambien redimensionamos las imagenes a un tamaño de 64 x 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(f'Versión de TensorFlow: {tf.__version__}')\n",
    "print(f\"Dispositivo de entrenamiento: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"../datasets/lego_bricks\",\n",
    "    labels=None,\n",
    "    color_mode='grayscale',\n",
    "    image_size=(64, 64),\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    interpolation='bilinear'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos originales se dimensionan en el rango [0, 255] para indicar la intensidad del pixel. Al entrenar redes GAN ajustamos los datos al rango [-1, 1], de forma que podamos usar la función de activación tanh (tangente hiperbólica) en la capa final del generador, que tiende a ofrecer gradientes más fuertes que la función sigmoide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    return (tf.cast(img, \"float32\") - 127.5) / 127.5\n",
    "\n",
    "train = train_data.map(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminador\n",
    "El objetivo del discriminador es predecir si una imagen es real o falsa. Es un problema de clasificación de imagen supervisada.\n",
    "\n",
    "Conviene observar que utilizamos un stride de 2 en algunas de las capas Conv2D para reducir la forma espacial del tensor cuando pasa por la red, aumentando al mismo tiempo el numero de canales. Usamos la función de activación *sigmoid* en la capa Conv2D final para producir un número entre 0 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_input = tf.keras.layers.Input(shape=(64, 64, 1))\n",
    "\n",
    "# Bloque 01\n",
    "x = tf.keras.layers.Conv2D(64, kernel_size=4, strides=2, padding='same', use_bias=False)(discriminator_input)\n",
    "x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "# Bloque 02\n",
    "x = tf.keras.layers.Conv2D(128, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "# Bloque 03\n",
    "x = tf.keras.layers.Conv2D(256, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "# Bloque 04\n",
    "x = tf.keras.layers.Conv2D(512, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "# Bloque 05\n",
    "x = tf.keras.layers.Conv2D(1, kernel_size=4, strides=1, padding='valid', use_bias=False, activation='sigmoid')(x)\n",
    "\n",
    "discrimnator_output = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "discriminator = tf.keras.models.Model(discriminator_input, discrimnator_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generador\n",
    "\n",
    "La entrada para el generador será un vector tomado de una distribución normal estandar multivariada. El resultado o salida es una imagen que tiene el mismo tamaño que una imagen de los datos de entrenamiento originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input = tf.keras.layers.Input(shape=(100, ))\n",
    "\n",
    "# Bloque 01\n",
    "x = tf.keras.layers.Reshape((1, 1, 100))(generator_input)\n",
    "x = tf.keras.layers.Conv2DTranspose(512, kernel_size=4, strides=1, padding='valid', use_bias=False)(x)\n",
    "x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "# Bloque 02\n",
    "x = tf.keras.layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "# Bloque 03\n",
    "x = tf.keras.layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "# Bloque 04\n",
    "x = tf.keras.layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "generator_output = tf.keras.layers.Conv2DTranspose(1, kernel_size=4, strides=2, padding='same', use_bias=False, activation='tanh')(x)\n",
    "\n",
    "generator = tf.keras.models.Model(generator_input, generator_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride\n",
    "\n",
    "Resumidamente, lo que hace el parametro *stride* es \"cortar\" las dimensiones de la entrada. Por ejemplo, si tenemos una imagen de 128 x 128 y aplicamos un *stride* de 2 obtendremos una imagen de 64 x 64. En el caso de usar una capa convolucional transpuesta el efecto será el contrario, ampliaremos el tamaño de la imagen en 2x.\n",
    "\n",
    "La información se ha sacado de esta [respuesta de StackOverflow](https://stackoverflow.com/questions/65440993/how-strides-effect-input-shapes-in-keras) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling2D vs Conv2DTranspose\n",
    "\n",
    "Se pueden usar las capas Upsampling2D en vez de las Conv2DTranspose. Por ejemplo:\n",
    "\n",
    "``` python\n",
    "x = tf.keras.layers.Upsampling2D(size=2)(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(256, kernel_size=4, strides=1, padding='same')(x)\n",
    "```\n",
    "\n",
    "La capa Upsampling2D repite cada fila y cada columna de su entrada para duplicar el tamaño. La capa Conv2D con *stride* 1 realiza la operación de convolución. Es una idea similar a la transposición convolucional, pero en lugar de rellenar los espacios entre pixeles con ceros el *upsampling* tan solo repite los valores de pixel existentes.\n",
    "\n",
    "Se ha demostrado que Conv2DTranspose puede producir artefactos. Lo ideal sería probar ambos métodos y verificar cual produce mejores resultados para según que objetivo.\n",
    "\n",
    "#### Enlaces interesantes\n",
    "\n",
    "- [An Introduction to different Types of Convolutions in Deep Learning](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar la red DCGAN\n",
    "\n",
    "Podemos entrenar al discriminador creando un conjunto de entrenamiento, en el que algunas de las imágenes son observaciones reales del conjunto y otras son resultados falsos del generador. Después tratamos esto como un problema de aprendizaje supervisado, donde las etiquetas son 1 para las imagenes reales y 0 para las falsas, con entropía cruzada binaria como función de pérdida.\n",
    "\n",
    "Para entrenar al generador debemos encontrar una forma de puntuar cada imagen generada, de forma que pueda optimizar hacia imágenes con puntuación alta. Podemos generar un grupo de imagenes y pasarlas por el discriminador para obtener una puntuación por cada imagen. La función de pérdida del generador es simplemente la entropia cruzada binaria entre estas probabilidades y un vector de unos, porque queremos entrenar al generador para producir imágenes que el discriminador piensa que son reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(tf.keras.models.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim) -> None:\n",
    "        super(DCGAN, self).__init__()\n",
    "        self.discriminator = discriminator \n",
    "        self.generator = generator \n",
    "        self.latent_dim = latent_dim \n",
    "        \n",
    "    def compile(self, discriminator_optimizer, generator_optimizer):\n",
    "        super(DCGAN, self).compile()\n",
    "        self.loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "        self.discriminator_optimizer = discriminator_optimizer\n",
    "        self.generator_optimizer = generator_optimizer\n",
    "        self.discriminator_loss_metric = tf.keras.metrics.Mean(name = 'discriminator_loss')\n",
    "        self.generator_loss_metric = tf.keras.metrics.Mean(name = 'generator_loss')\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.discriminator_loss_metric, self.generator_loss_metric]\n",
    "    \n",
    "    def train_step(self, real_images):\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        \n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: \n",
    "            generated_images = self.generator(random_latent_vectors, training=True)\n",
    "            \n",
    "            real_predictions = self.discriminator(real_images, training=True) \n",
    "            fake_predictions = self.discriminator(generated_images, training=True) \n",
    "            \n",
    "            real_labels = tf.ones_like(real_predictions)\n",
    "            real_noisy_labels = real_labels + 0.1 * tf.random.uniform(tf.shape(real_predictions))\n",
    "            \n",
    "            fake_labels = tf.zeros_like(fake_predictions)\n",
    "            fake_noisy_labels = fake_labels - 0.1 * tf.random.uniform(tf.shape(fake_predictions))\n",
    "            \n",
    "            discriminator_real_loss = self.loss_fn(real_noisy_labels, real_predictions)\n",
    "            discriminator_fake_loss = self.loss_fn(fake_noisy_labels, fake_predictions)\n",
    "            discriminator_loss = (discriminator_real_loss + discriminator_fake_loss) / 2.0\n",
    "            \n",
    "            generator_loss = self.loss_fn(real_labels, fake_predictions)\n",
    "        \n",
    "        gradients_of_discriminator = disc_tape.gradient(discriminator_loss, self.discriminator.trainable_variables)\n",
    "        gradients_of_generator = gen_tape.gradient(generator_loss, self.generator.trainable_variables)\n",
    "        \n",
    "        self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "        self.generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "        \n",
    "        self.discriminator_loss_metric.update_state(discriminator_loss)\n",
    "        self.generator_loss_metric.update_state(generator_loss)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcgan = DCGAN(discriminator=discriminator, generator=generator, latent_dim=100)\n",
    "dcgan.compile(\n",
    "    discriminator_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999),\n",
    "    generator_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n",
    ")\n",
    "dcgan.fit(train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_images(img1, img2):\n",
    "    return np.mean(np.abs(img1 - img2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
