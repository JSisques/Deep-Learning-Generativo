{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El conjunto de datos CelebA \n",
    "\n",
    "Utilizaremos el conjunto de datos CelebFaces Attributes para entrenar nuestro modelo autocodificador variacional. Se puede encontrar en Kaggle, [CelebA](https://www.kaggle.com/datasets/jessicali9530/celeba-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de TensorFlow: 2.10.0\n",
      "Dispositivo de entrenamiento: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "print(f'Versión de TensorFlow: {tf.__version__}')\n",
    "print(f\"Dispositivo de entrenamiento: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 128\n",
    "NUM_FEATURES = 128\n",
    "Z_DIM = 200\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 10\n",
    "BETA = 2000\n",
    "LOAD_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(dataset):\n",
    "    batch = dataset.take(1).get_single_element()\n",
    "    if isinstance(batch, tuple):\n",
    "        batch = batch[0]\n",
    "    return batch.numpy()\n",
    "\n",
    "\n",
    "def display(\n",
    "    images, n=10, size=(20, 3), cmap=\"gray_r\", as_type=\"float32\", save_to=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Displays n random images from each one of the supplied arrays.\n",
    "    \"\"\"\n",
    "    if images.max() > 1.0:\n",
    "        images = images / 255.0\n",
    "    elif images.min() < 0.0:\n",
    "        images = (images + 1.0) / 2.0\n",
    "\n",
    "    plt.figure(figsize=size)\n",
    "    for i in range(n):\n",
    "        _ = plt.subplot(1, n, i + 1)\n",
    "        plt.imshow(images[i].astype(as_type), cmap=cmap)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    if save_to:\n",
    "        plt.savefig(save_to)\n",
    "        print(f\"\\nSaved to {save_to}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_vector_from_label(data, vae, embedding_dim, label):\n",
    "    current_sum_POS = np.zeros(shape=embedding_dim, dtype=\"float32\")\n",
    "    current_n_POS = 0\n",
    "    current_mean_POS = np.zeros(shape=embedding_dim, dtype=\"float32\")\n",
    "\n",
    "    current_sum_NEG = np.zeros(shape=embedding_dim, dtype=\"float32\")\n",
    "    current_n_NEG = 0\n",
    "    current_mean_NEG = np.zeros(shape=embedding_dim, dtype=\"float32\")\n",
    "\n",
    "    current_vector = np.zeros(shape=embedding_dim, dtype=\"float32\")\n",
    "    current_dist = 0\n",
    "\n",
    "    print(\"label: \" + label)\n",
    "    print(\"images : POS move : NEG move :distance : 𝛥 distance\")\n",
    "    while current_n_POS < 10000:\n",
    "        batch = list(data.take(1).get_single_element())\n",
    "        im = batch[0]\n",
    "        attribute = batch[1]\n",
    "\n",
    "        _, _, z = vae.encoder.predict(np.array(im), verbose=0)\n",
    "\n",
    "        z_POS = z[attribute == 1]\n",
    "        z_NEG = z[attribute == -1]\n",
    "\n",
    "        if len(z_POS) > 0:\n",
    "            current_sum_POS = current_sum_POS + np.sum(z_POS, axis=0)\n",
    "            current_n_POS += len(z_POS)\n",
    "            new_mean_POS = current_sum_POS / current_n_POS\n",
    "            movement_POS = np.linalg.norm(new_mean_POS - current_mean_POS)\n",
    "\n",
    "        if len(z_NEG) > 0:\n",
    "            current_sum_NEG = current_sum_NEG + np.sum(z_NEG, axis=0)\n",
    "            current_n_NEG += len(z_NEG)\n",
    "            new_mean_NEG = current_sum_NEG / current_n_NEG\n",
    "            movement_NEG = np.linalg.norm(new_mean_NEG - current_mean_NEG)\n",
    "\n",
    "        current_vector = new_mean_POS - new_mean_NEG\n",
    "        new_dist = np.linalg.norm(current_vector)\n",
    "        dist_change = new_dist - current_dist\n",
    "\n",
    "        print(\n",
    "            str(current_n_POS)\n",
    "            + \"    : \"\n",
    "            + str(np.round(movement_POS, 3))\n",
    "            + \"    : \"\n",
    "            + str(np.round(movement_NEG, 3))\n",
    "            + \"    : \"\n",
    "            + str(np.round(new_dist, 3))\n",
    "            + \"    : \"\n",
    "            + str(np.round(dist_change, 3))\n",
    "        )\n",
    "\n",
    "        current_mean_POS = np.copy(new_mean_POS)\n",
    "        current_mean_NEG = np.copy(new_mean_NEG)\n",
    "        current_dist = np.copy(new_dist)\n",
    "\n",
    "        if np.sum([movement_POS, movement_NEG]) < 0.08:\n",
    "            current_vector = current_vector / current_dist\n",
    "            print(\"Found the \" + label + \" vector\")\n",
    "            break\n",
    "\n",
    "    return current_vector\n",
    "\n",
    "\n",
    "def add_vector_to_images(data, vae, feature_vec):\n",
    "    n_to_show = 5\n",
    "    factors = [-4, -3, -2, -1, 0, 1, 2, 3, 4]\n",
    "\n",
    "    example_batch = list(data.take(1).get_single_element())\n",
    "    example_images = example_batch[0]\n",
    "\n",
    "    _, _, z_points = vae.encoder.predict(example_images, verbose=0)\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "    counter = 1\n",
    "\n",
    "    for i in range(n_to_show):\n",
    "        img = example_images[i]\n",
    "        sub = fig.add_subplot(n_to_show, len(factors) + 1, counter)\n",
    "        sub.axis(\"off\")\n",
    "        sub.imshow(img)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "        for factor in factors:\n",
    "            changed_z_point = z_points[i] + feature_vec * factor\n",
    "            changed_image = vae.decoder.predict(\n",
    "                np.array([changed_z_point]), verbose=0\n",
    "            )[0]\n",
    "\n",
    "            sub = fig.add_subplot(n_to_show, len(factors) + 1, counter)\n",
    "            sub.axis(\"off\")\n",
    "            sub.imshow(changed_image)\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def morph_faces(data, vae):\n",
    "    factors = np.arange(0, 1, 0.1)\n",
    "\n",
    "    example_batch = list(data.take(1).get_single_element())[:2]\n",
    "    example_images = example_batch[0]\n",
    "    _, _, z_points = vae.encoder.predict(example_images, verbose=0)\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 8))\n",
    "\n",
    "    counter = 1\n",
    "\n",
    "    img = example_images[0]\n",
    "    sub = fig.add_subplot(1, len(factors) + 2, counter)\n",
    "    sub.axis(\"off\")\n",
    "    sub.imshow(img)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    for factor in factors:\n",
    "        changed_z_point = z_points[0] * (1 - factor) + z_points[1] * factor\n",
    "        changed_image = vae.decoder.predict(\n",
    "            np.array([changed_z_point]), verbose=0\n",
    "        )[0]\n",
    "        sub = fig.add_subplot(1, len(factors) + 2, counter)\n",
    "        sub.axis(\"off\")\n",
    "        sub.imshow(changed_image)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    img = example_images[1]\n",
    "    sub = fig.add_subplot(1, len(factors) + 2, counter)\n",
    "    sub.axis(\"off\")\n",
    "    sub.imshow(img)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202599 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"../datasets/celebA/img\",\n",
    "    labels=None,\n",
    "    color_mode='rgb',\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    interpolation='bilinear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    return tf.cast(img, \"float32\") / 255.0\n",
    "\n",
    "train = train_data.map(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = sample_batch(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 16, 16, 128)  3584        ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 16, 16, 128)  512        ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 16, 16, 128)  0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 8, 8, 128)    147584      ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 8, 8, 128)   512         ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 8, 8, 128)    0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 4, 4, 128)    147584      ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 4, 4, 128)   512         ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 4, 4, 128)    0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 2, 2, 128)    147584      ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 2, 2, 128)   512         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 2, 2, 128)    0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 512)          0           ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 200)          102600      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 200)          102600      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 200)          0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 653,584\n",
      "Trainable params: 652,560\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "encoder_input = tf.keras.layers.Input(\n",
    "    shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS), name=\"encoder_input\"\n",
    ")\n",
    "x = tf.keras.layers.Conv2D(NUM_FEATURES, kernel_size=3, strides=2, padding=\"same\")(\n",
    "    encoder_input\n",
    ")\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU()(x)\n",
    "x = tf.keras.layers.Conv2D(NUM_FEATURES, kernel_size=3, strides=2, padding=\"same\")(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU()(x)\n",
    "x = tf.keras.layers.Conv2D(NUM_FEATURES, kernel_size=3, strides=2, padding=\"same\")(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU()(x)\n",
    "x = tf.keras.layers.Conv2D(NUM_FEATURES, kernel_size=3, strides=2, padding=\"same\")(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU()(x)\n",
    "shape_before_flattening = K.int_shape(x)[1:]  # the decoder will need this!\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "z_mean = tf.keras.layers.Dense(Z_DIM, name=\"z_mean\")(x)\n",
    "z_log_var = tf.keras.layers.Dense(Z_DIM, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "encoder = tf.keras.models.Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 200)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               102912    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 512)               0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 4, 4, 128)        147584    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 4, 4, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 8, 8, 128)        147584    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 16, 16, 128)      147584    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 16, 16, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 32, 32, 128)      147584    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 32, 32, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 32, 32, 3)        3459      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 700,803\n",
      "Trainable params: 698,755\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Decoder\n",
    "decoder_input = tf.keras.layers.Input(shape=(Z_DIM,), name=\"decoder_input\")\n",
    "x = tf.keras.layers.Dense(np.prod(shape_before_flattening))(decoder_input)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU()(x)\n",
    "x = tf.keras.layers.Reshape(shape_before_flattening)(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(\n",
    "    NUM_FEATURES, kernel_size=3, strides=2, padding=\"same\"\n",
    ")(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU()(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(\n",
    "    NUM_FEATURES, kernel_size=3, strides=2, padding=\"same\"\n",
    ")(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU()(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(\n",
    "    NUM_FEATURES, kernel_size=3, strides=2, padding=\"same\"\n",
    ")(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU()(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(\n",
    "    NUM_FEATURES, kernel_size=3, strides=2, padding=\"same\"\n",
    ")(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU()(x)\n",
    "decoder_output = tf.keras.layers.Conv2DTranspose(\n",
    "    CHANNELS, kernel_size=3, strides=1, activation=\"sigmoid\", padding=\"same\"\n",
    ")(x)\n",
    "decoder = tf.keras.models.Model(decoder_input, decoder_output)\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.models.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Call the model on a particular input.\"\"\"\n",
    "        z_mean, z_log_var, z = encoder(inputs)\n",
    "        reconstruction = decoder(z)\n",
    "        return z_mean, z_log_var, reconstruction\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\"Step run during training.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, reconstruction = self(data, training=True)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                BETA * tf.keras.losses.mean_squared_error(data, reconstruction)\n",
    "            )\n",
    "            kl_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    -0.5\n",
    "                    * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),\n",
    "                    axis=1,\n",
    "                )\n",
    "            )\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        \"\"\"Step run during validation.\"\"\"\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "\n",
    "        z_mean, z_log_var, reconstruction = self(data)\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            BETA * tf.keras.losses.mean_squared_error(data, reconstruction)\n",
    "        )\n",
    "        kl_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),\n",
    "                axis=1,\n",
    "            )\n",
    "        )\n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "vae.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model save checkpoint\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"./checkpoint\",\n",
    "    save_weights_only=False,\n",
    "    save_freq=\"epoch\",\n",
    "    monitor=\"loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "\n",
    "class ImageGenerator(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, num_img, latent_dim):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(\n",
    "            shape=(self.num_img, self.latent_dim)\n",
    "        )\n",
    "        generated_images = self.model.decoder(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = tf.keras.utils.array_to_img(generated_images[i])\n",
    "            img.save(\"./output/generated_img_%03d_%d.png\" % (epoch, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load old weights if required\n",
    "if LOAD_MODEL:\n",
    "    vae.load_weights(\"./models/vae\")\n",
    "    tmp = vae.predict(train.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Javie\\Documents\\Proyectos\\JSisques\\Deep-Learning-Generativo\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Javie\\Documents\\Proyectos\\JSisques\\Deep-Learning-Generativo\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Javie\\Documents\\Proyectos\\JSisques\\Deep-Learning-Generativo\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Javie\\AppData\\Local\\Temp\\ipykernel_16208\\1055918116.py\", line 29, in train_step\n        z_mean, z_log_var, reconstruction = self(data, training=True)\n    File \"c:\\Users\\Javie\\Documents\\Proyectos\\JSisques\\Deep-Learning-Generativo\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Javie\\AppData\\Local\\Temp\\__autograph_generated_fileux1pkkws.py\", line 11, in tf__call\n        (z_mean, z_log_var, z) = ag__.converted_call(ag__.ld(encoder), (ag__.ld(inputs),), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"vae\" \"                 f\"(type VAE).\n    \n    in user code:\n    \n        File \"C:\\Users\\Javie\\AppData\\Local\\Temp\\ipykernel_16208\\1055918116.py\", line 22, in call  *\n            z_mean, z_log_var, z = encoder(inputs)\n        File \"c:\\Users\\Javie\\Documents\\Proyectos\\JSisques\\Deep-Learning-Generativo\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"c:\\Users\\Javie\\Documents\\Proyectos\\JSisques\\Deep-Learning-Generativo\\.venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n            raise ValueError(\n    \n        ValueError: Input 0 of layer \"encoder\" is incompatible with the layer: expected shape=(None, 32, 32, 3), found shape=(None, 64, 64, 3)\n    \n    \n    Call arguments received by layer \"vae\" \"                 f\"(type VAE):\n      • inputs=tf.Tensor(shape=(None, 64, 64, 3), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mImageGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mZ_DIM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Javie\\Documents\\Proyectos\\JSisques\\Deep-Learning-Generativo\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileuc_bmgqm.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m, in \u001b[0;36mVAE.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Step run during training.\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 29\u001b[0m     z_mean, z_log_var, reconstruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(data, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m     reconstruction_loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(\n\u001b[0;32m     31\u001b[0m         BETA \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mmean_squared_error(data, reconstruction)\n\u001b[0;32m     32\u001b[0m     )\n\u001b[0;32m     33\u001b[0m     kl_loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(\n\u001b[0;32m     34\u001b[0m         tf\u001b[38;5;241m.\u001b[39mreduce_sum(\n\u001b[0;32m     35\u001b[0m             \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m         )\n\u001b[0;32m     39\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileux1pkkws.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      9\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     10\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 11\u001b[0m (z_mean, z_log_var, z) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(encoder), (ag__\u001b[38;5;241m.\u001b[39mld(inputs),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m reconstruction \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(decoder), (ag__\u001b[38;5;241m.\u001b[39mld(z),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Javie\\Documents\\Proyectos\\JSisques\\Deep-Learning-Generativo\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Javie\\Documents\\Proyectos\\JSisques\\Deep-Learning-Generativo\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Javie\\Documents\\Proyectos\\JSisques\\Deep-Learning-Generativo\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Javie\\AppData\\Local\\Temp\\ipykernel_16208\\1055918116.py\", line 29, in train_step\n        z_mean, z_log_var, reconstruction = self(data, training=True)\n    File \"c:\\Users\\Javie\\Documents\\Proyectos\\JSisques\\Deep-Learning-Generativo\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Javie\\AppData\\Local\\Temp\\__autograph_generated_fileux1pkkws.py\", line 11, in tf__call\n        (z_mean, z_log_var, z) = ag__.converted_call(ag__.ld(encoder), (ag__.ld(inputs),), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"vae\" \"                 f\"(type VAE).\n    \n    in user code:\n    \n        File \"C:\\Users\\Javie\\AppData\\Local\\Temp\\ipykernel_16208\\1055918116.py\", line 22, in call  *\n            z_mean, z_log_var, z = encoder(inputs)\n        File \"c:\\Users\\Javie\\Documents\\Proyectos\\JSisques\\Deep-Learning-Generativo\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"c:\\Users\\Javie\\Documents\\Proyectos\\JSisques\\Deep-Learning-Generativo\\.venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n            raise ValueError(\n    \n        ValueError: Input 0 of layer \"encoder\" is incompatible with the layer: expected shape=(None, 32, 32, 3), found shape=(None, 64, 64, 3)\n    \n    \n    Call arguments received by layer \"vae\" \"                 f\"(type VAE):\n      • inputs=tf.Tensor(shape=(None, 64, 64, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "vae.fit(\n",
    "    train,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[\n",
    "        model_checkpoint_callback,\n",
    "        tensorboard_callback,\n",
    "        ImageGenerator(num_img=10, latent_dim=Z_DIM),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final models\n",
    "vae.save(\"./models/capitulo_03/vae\")\n",
    "encoder.save(\"./models/capitulo_03/encoder\")\n",
    "decoder.save(\"./models/capitulo_03/decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of the test set\n",
    "batches_to_predict = 1\n",
    "example_images = np.array(\n",
    "    list(train.take(batches_to_predict).get_single_element())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create autoencoder predictions and display\n",
    "z_mean, z_log_var, reconstructions = vae.predict(example_images)\n",
    "print(\"Example real faces\")\n",
    "display(example_images)\n",
    "print(\"Reconstructions\")\n",
    "display(reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, z = vae.encoder.predict(example_images)\n",
    "\n",
    "x = np.linspace(-3, 3, 100)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "fig.subplots_adjust(hspace=0.6, wspace=0.4)\n",
    "\n",
    "for i in range(50):\n",
    "    ax = fig.add_subplot(5, 10, i + 1)\n",
    "    ax.hist(z[:, i], density=True, bins=20)\n",
    "    ax.axis(\"off\")\n",
    "    ax.text(\n",
    "        0.5, -0.35, str(i), fontsize=10, ha=\"center\", transform=ax.transAxes\n",
    "    )\n",
    "    ax.plot(x, norm.pdf(x))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some points in the latent space, from the standard normal distribution\n",
    "grid_width, grid_height = (10, 3)\n",
    "z_sample = np.random.normal(size=(grid_width * grid_height, Z_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decode the sampled points\n",
    "reconstructions = decoder.predict(z_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a plot of decoded images\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "# Output the grid of faces\n",
    "for i in range(grid_width * grid_height):\n",
    "    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(reconstructions[i, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the label dataset\n",
    "attributes = pd.read_csv(\"../datasets/celebA/list_attr_celeba.csv\")\n",
    "print(attributes.columns)\n",
    "attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the face data with label attached\n",
    "LABEL = \"Blond_Hair\"  # <- Set this label\n",
    "labelled_test = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"../datasets/celebA/img\",\n",
    "    labels=attributes[LABEL].tolist(),\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    interpolation=\"bilinear\",\n",
    ")\n",
    "\n",
    "labelled = labelled_test.map(lambda x, y: (preprocess(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the attribute vector\n",
    "attribute_vec = get_vector_from_label(labelled, vae, Z_DIM, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add vector to images\n",
    "add_vector_to_images(labelled, vae, attribute_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_faces(labelled, vae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
